{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English to Japanese NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQY9UGUHPCxe"
      },
      "source": [
        "As the written Japanese (Kanji) is different from written Englsih, I have used Janome library to tokenize the Japanese sentences in the given dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtOt_zC7U0k8",
        "outputId": "7e80ad36-c061-42c2-c43b-898a672ee9e3"
      },
      "source": [
        "!pip install Janome"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Janome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7MB 1.3MB/s \n",
            "\u001b[?25hInstalling collected packages: Janome\n",
            "Successfully installed Janome-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KL7Z2EOhT5w"
      },
      "source": [
        "#importing required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from string import digits\n",
        "from sklearn.utils import shuffle\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import to_categorical\n",
        "from tqdm import tqdm\n",
        "\n",
        "#tokenizer for japanese sequences\n",
        "from janome.tokenizer import Tokenizer as janome_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "1NlzwerQQy_O",
        "outputId": "186fa2e7-9a62-4f85-8964-e9864321b377"
      },
      "source": [
        "#loading the dataset into the notebook\n",
        "from google.colab import files\n",
        "data = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d1d7fdc4-adc3-4f88-ba44-0e45af5d2f7b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d1d7fdc4-adc3-4f88-ba44-0e45af5d2f7b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving trainset.csv to trainset.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvfXk51olm87"
      },
      "source": [
        "#Loading the dataset as dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIJ0WvSEct5C"
      },
      "source": [
        "#loading the dataset as df\n",
        "df = pd.read_csv('trainset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "BMSHkaWbJyZQ",
        "outputId": "a0ce4630-6442-4739-ae05-ef0dc8ac2a65"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>eng</th>\n",
              "      <th>jp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>my opponent is shark.</td>\n",
              "      <td>俺の相手は シャークだ。</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>this is one thing in exchange for another.</td>\n",
              "      <td>引き換えだ ある事とある物の</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>yeah, i'm fine.</td>\n",
              "      <td>もういいよ ごちそうさま ううん</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>don't come to the office anymore. don't call m...</td>\n",
              "      <td>もう会社には来ないでくれ 電話もするな</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>looks beautiful.</td>\n",
              "      <td>きれいだ。</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33819</th>\n",
              "      <td>33819</td>\n",
              "      <td>where are you?</td>\n",
              "      <td>どこに居る?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33820</th>\n",
              "      <td>33820</td>\n",
              "      <td>i'm assuming you have a little more time, you ...</td>\n",
              "      <td>まだ時間があると思ってるんだ ちょっと黙ってろ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33821</th>\n",
              "      <td>33821</td>\n",
              "      <td>nickleby?</td>\n",
              "      <td>害虫退治です アリを焼いております</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33822</th>\n",
              "      <td>33822</td>\n",
              "      <td>look at me. you don't look right to me.</td>\n",
              "      <td>私を見ろ - 俺を見るな</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33823</th>\n",
              "      <td>33823</td>\n",
              "      <td>you must be...</td>\n",
              "      <td>. - ダニエル - 調子は?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>33824 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  ...                       jp\n",
              "0               0  ...             俺の相手は シャークだ。\n",
              "1               1  ...           引き換えだ ある事とある物の\n",
              "2               2  ...         もういいよ ごちそうさま ううん\n",
              "3               3  ...      もう会社には来ないでくれ 電話もするな\n",
              "4               4  ...                    きれいだ。\n",
              "...           ...  ...                      ...\n",
              "33819       33819  ...                   どこに居る?\n",
              "33820       33820  ...  まだ時間があると思ってるんだ ちょっと黙ってろ\n",
              "33821       33821  ...        害虫退治です アリを焼いております\n",
              "33822       33822  ...             私を見ろ - 俺を見るな\n",
              "33823       33823  ...          . - ダニエル - 調子は?\n",
              "\n",
              "[33824 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43kVg0RPLHHN"
      },
      "source": [
        "#EDA and Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8G_5BP5LGlC",
        "outputId": "65a4f2e9-7748-4e19-9664-52c54a8a64c4"
      },
      "source": [
        "#getting the shape of data\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(33824, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_toLtjvanf1"
      },
      "source": [
        "#droping the index column from the train set\n",
        "df = df.drop(['Unnamed: 0'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvvebX-Gbjcx",
        "scrolled": true,
        "outputId": "90e2fd7b-b184-4861-bb49-c213a27d1db0"
      },
      "source": [
        "#checking for null values in the corpus\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "eng    0\n",
              "jp     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxN6taxTdZlB"
      },
      "source": [
        "#turning all words to lower-case in training dataset\n",
        "#for english sequences\n",
        "df['eng']=df['eng'].apply(lambda x: x.lower())\n",
        "#for japanese sequences\n",
        "df['jp']=df['jp'].apply(lambda x: x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtvbOu1KebmR"
      },
      "source": [
        "#removing all punctuations and special characters from the datasets\n",
        "remove_punc = set(string.punctuation)\n",
        "#since the japanese punctuation is different, making a separate list\n",
        "rem_jp_punc = set('、。【】「」『』…・〽（）〜？！｡：､；･')\n",
        "\n",
        "#removing all the punctuations and special characters in training dataset\n",
        "df['eng']=df['eng'].apply(lambda x: ''.join(ch for ch in x if ch not in remove_punc))\n",
        "df['jp']=df['jp'].apply(lambda x: ''.join(ch for ch in x if ch not in remove_punc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "yRmjvjZwng3O",
        "outputId": "482e41e9-8d03-4f22-b376-590be897480c"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>jp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>my opponent is shark</td>\n",
              "      <td>俺の相手は シャークだ。</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>this is one thing in exchange for another</td>\n",
              "      <td>引き換えだ ある事とある物の</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>yeah im fine</td>\n",
              "      <td>もういいよ ごちそうさま ううん</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dont come to the office anymore dont call me e...</td>\n",
              "      <td>もう会社には来ないでくれ 電話もするな</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>looks beautiful</td>\n",
              "      <td>きれいだ。</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33819</th>\n",
              "      <td>where are you</td>\n",
              "      <td>どこに居る</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33820</th>\n",
              "      <td>im assuming you have a little more time you in...</td>\n",
              "      <td>まだ時間があると思ってるんだ ちょっと黙ってろ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33821</th>\n",
              "      <td>nickleby</td>\n",
              "      <td>害虫退治です アリを焼いております</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33822</th>\n",
              "      <td>look at me you dont look right to me</td>\n",
              "      <td>私を見ろ  俺を見るな</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33823</th>\n",
              "      <td>you must be</td>\n",
              "      <td>ダニエル  調子は</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>33824 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     eng                       jp\n",
              "0                                   my opponent is shark             俺の相手は シャークだ。\n",
              "1              this is one thing in exchange for another           引き換えだ ある事とある物の\n",
              "2                                           yeah im fine         もういいよ ごちそうさま ううん\n",
              "3      dont come to the office anymore dont call me e...      もう会社には来ないでくれ 電話もするな\n",
              "4                                        looks beautiful                    きれいだ。\n",
              "...                                                  ...                      ...\n",
              "33819                                      where are you                    どこに居る\n",
              "33820  im assuming you have a little more time you in...  まだ時間があると思ってるんだ ちょっと黙ってろ\n",
              "33821                                           nickleby        害虫退治です アリを焼いております\n",
              "33822               look at me you dont look right to me              私を見ろ  俺を見るな\n",
              "33823                                        you must be                ダニエル  調子は\n",
              "\n",
              "[33824 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XTlijnAfxRO"
      },
      "source": [
        "As we can see from the output above, all punctuations and special characters have been removed from both the datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAs5qZm9ikaa"
      },
      "source": [
        "#removing numbers, if present, in the datasets\n",
        "remove_dig = str.maketrans('', '', digits)\n",
        "df['eng']=df['eng'].apply(lambda x: x.translate(remove_dig))\n",
        "df['jp']=df['jp'].apply(lambda x: x.translate(remove_dig))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8hX-dSvL41I"
      },
      "source": [
        "#adding the length of each sequence in the training dataset\n",
        "df['len_eng_seq']=df['eng'].apply(lambda x:len(x.split(\" \")))\n",
        "df['len_jp_seq']=df['jp'].apply(lambda x:len(x.split(\" \")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "EibM9ElKL41I",
        "outputId": "1e2129cf-7ecd-4878-b825-01cff936b183"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>jp</th>\n",
              "      <th>len_eng_seq</th>\n",
              "      <th>len_jp_seq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>my opponent is shark</td>\n",
              "      <td>俺の相手は シャークだ。</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>this is one thing in exchange for another</td>\n",
              "      <td>引き換えだ ある事とある物の</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>yeah im fine</td>\n",
              "      <td>もういいよ ごちそうさま ううん</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dont come to the office anymore dont call me e...</td>\n",
              "      <td>もう会社には来ないでくれ 電話もするな</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>looks beautiful</td>\n",
              "      <td>きれいだ。</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33819</th>\n",
              "      <td>where are you</td>\n",
              "      <td>どこに居る</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33820</th>\n",
              "      <td>im assuming you have a little more time you in...</td>\n",
              "      <td>まだ時間があると思ってるんだ ちょっと黙ってろ</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33821</th>\n",
              "      <td>nickleby</td>\n",
              "      <td>害虫退治です アリを焼いております</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33822</th>\n",
              "      <td>look at me you dont look right to me</td>\n",
              "      <td>私を見ろ  俺を見るな</td>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33823</th>\n",
              "      <td>you must be</td>\n",
              "      <td>ダニエル  調子は</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>33824 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     eng  ... len_jp_seq\n",
              "0                                   my opponent is shark  ...          2\n",
              "1              this is one thing in exchange for another  ...          2\n",
              "2                                           yeah im fine  ...          3\n",
              "3      dont come to the office anymore dont call me e...  ...          2\n",
              "4                                        looks beautiful  ...          1\n",
              "...                                                  ...  ...        ...\n",
              "33819                                      where are you  ...          1\n",
              "33820  im assuming you have a little more time you in...  ...          2\n",
              "33821                                           nickleby  ...          2\n",
              "33822               look at me you dont look right to me  ...          3\n",
              "33823                                        you must be  ...          5\n",
              "\n",
              "[33824 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY2L7XJwT-p8"
      },
      "source": [
        "#initializing the tokenizer\n",
        "token_jp = janome_tokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpdmZvFpVNQ1",
        "outputId": "b5bc2d9a-5f57-4d08-808c-5392d519381a"
      },
      "source": [
        "#applying to japanese sentences in the dataset\n",
        "df['jp'] = [' '.join([word for word in token_jp.tokenize(x, wakati=True) \\\n",
        "                      if word != ' ']) for x in tqdm(df['jp'])]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 33824/33824 [01:00<00:00, 558.11it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPCOes4KZ85H"
      },
      "source": [
        "#splitting english sentences into words\n",
        "df['eng'] =df['eng'].apply(lambda row: row.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD-X81zzbkyx"
      },
      "source": [
        "#splitting japanese sentences into words\n",
        "df['jp']=df['jp'].apply(lambda row: row.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "9P_5oIh0a8OD",
        "outputId": "d6b8cbf4-f659-4d99-f2e2-98594a1e2add"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>jp</th>\n",
              "      <th>len_eng_seq</th>\n",
              "      <th>len_jp_seq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[my, opponent, is, shark]</td>\n",
              "      <td>[俺, の, 相手, は, シャーク, だ, 。]</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[this, is, one, thing, in, exchange, for, anot...</td>\n",
              "      <td>[引き換え, だ, ある, 事, と, ある, 物, の]</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[yeah, im, fine]</td>\n",
              "      <td>[もう, いい, よ, ごちそうさま, ううん]</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[dont, come, to, the, office, anymore, dont, c...</td>\n",
              "      <td>[もう, 会社, に, は, 来, ない, で, くれ, 電話, も, する, な]</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[looks, beautiful]</td>\n",
              "      <td>[きれい, だ, 。]</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33819</th>\n",
              "      <td>[where, are, you]</td>\n",
              "      <td>[どこ, に, 居る]</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33820</th>\n",
              "      <td>[im, assuming, you, have, a, little, more, tim...</td>\n",
              "      <td>[まだ, 時間, が, ある, と, 思っ, てる, ん, だ, ちょっと, 黙っ, てろ]</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33821</th>\n",
              "      <td>[nickleby]</td>\n",
              "      <td>[害虫, 退治, です, アリ, を, 焼い, て, おり, ます]</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33822</th>\n",
              "      <td>[look, at, me, you, dont, look, right, to, me]</td>\n",
              "      <td>[私, を, 見ろ, 俺, を, 見る, な]</td>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33823</th>\n",
              "      <td>[you, must, be]</td>\n",
              "      <td>[ダニエル, 調子, は]</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>33824 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     eng  ... len_jp_seq\n",
              "0                              [my, opponent, is, shark]  ...          2\n",
              "1      [this, is, one, thing, in, exchange, for, anot...  ...          2\n",
              "2                                       [yeah, im, fine]  ...          3\n",
              "3      [dont, come, to, the, office, anymore, dont, c...  ...          2\n",
              "4                                     [looks, beautiful]  ...          1\n",
              "...                                                  ...  ...        ...\n",
              "33819                                  [where, are, you]  ...          1\n",
              "33820  [im, assuming, you, have, a, little, more, tim...  ...          2\n",
              "33821                                         [nickleby]  ...          2\n",
              "33822     [look, at, me, you, dont, look, right, to, me]  ...          3\n",
              "33823                                    [you, must, be]  ...          5\n",
              "\n",
              "[33824 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opxz6WmyNKi_"
      },
      "source": [
        "The above output shows how each word has been separated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Zkqa7jbY7l"
      },
      "source": [
        "#removing all rows where the english sentence exceeds 6 words\n",
        "df=df[df['len_eng_seq']<=6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8-o43mIL41L"
      },
      "source": [
        "#removing all rows where the japanese sentence exceeds 6 words\n",
        "df=df[df['len_jp_seq']<=6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPJHH7BRdwtO",
        "outputId": "2cc507cd-37fe-46b6-dbd3-258267d225b3"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18066, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTyvBVUaPvdQ"
      },
      "source": [
        "I have preformed the step above becasue without this filter, the dataset was too heavy for my computer to handle. I have only kept the sentences that are not longer than 6 words for both English and Japanese."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqjLCG1iL41M"
      },
      "source": [
        "#saving the sentences into X and y\n",
        "X = df['jp'].values\n",
        "y = df['eng'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_70jtOIBdsAd"
      },
      "source": [
        "#getting the vocabulary for english and japanese sequences by tokenizing using Keras Tokenizer\n",
        "#tokenizing eng sentences\n",
        "eng_tokenizer = Tokenizer()\n",
        "eng_tokenizer.fit_on_texts(y)\n",
        "\n",
        "#tokenizing japanese sentences\n",
        "jp_tokenizer = Tokenizer()\n",
        "jp_tokenizer.fit_on_texts(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t4PCb5n6t2k"
      },
      "source": [
        "#getting vocab size for english\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1 \n",
        "\n",
        "#getting vocab size for japanese\n",
        "jp_vocab_size = len(jp_tokenizer.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dC8aGb6jeLTR",
        "outputId": "f8ab4932-e408-4d92-b812-fe534f51a09f"
      },
      "source": [
        "print(f'English vocab size:', eng_vocab_size)\n",
        "print(f'Japanese vocab size:', jp_vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English vocab size: 9621\n",
            "Japanese vocab size: 12739\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYhQbQ-mL41N"
      },
      "source": [
        "#getting max length for the longest japanese sentence\n",
        "jp_len = max(df['len_jp_seq'])\n",
        "#getting max length for the longest english sentence\n",
        "eng_len = max(df['len_eng_seq'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9i4KqkmL41O"
      },
      "source": [
        "#splitting the data into train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-x3t_ANL41O",
        "outputId": "5114203e-9870-48cc-9b1e-4f5726e4afcf"
      },
      "source": [
        "#printing the shapes of test and train data\n",
        "print('Size of X_train', X_train.shape)\n",
        "print('Size of y_train', y_train.shape)\n",
        "print('Size of X_test', X_test.shape)\n",
        "print('Size of y_test', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of X_train (16259,)\n",
            "Size of y_train (16259,)\n",
            "Size of X_test (1807,)\n",
            "Size of y_test (1807,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlNzvoOBgR9g"
      },
      "source": [
        "Converting English and Japanese sentences into sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1j8CG00gYnt"
      },
      "source": [
        "#japanese sentences to sequences\n",
        "X_train = jp_tokenizer.texts_to_sequences(X_train)\n",
        "X_test = jp_tokenizer.texts_to_sequences(X_test)\n",
        "#englsih sentences to sequences\n",
        "y_train = eng_tokenizer.texts_to_sequences(y_train)\n",
        "y_test = eng_tokenizer.texts_to_sequences(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abGQ3ZlPlfhh"
      },
      "source": [
        "#padding the sequences\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen = jp_len)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen = jp_len)\n",
        "y_train = pad_sequences(y_train, padding='post', maxlen = eng_len)\n",
        "y_test = pad_sequences(y_test, padding='post', maxlen = eng_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A4COEwcREmH"
      },
      "source": [
        "#function to one-hot encode y_train and y_test\n",
        "def encode_output(sequences, vocab_size):\n",
        "    ylist = list()\n",
        "    for seq in sequences:\n",
        "        encoded = to_categorical(seq, num_classes=vocab_size)\n",
        "        ylist.append(encoded)\n",
        "    y = np.array(ylist)\n",
        "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khbElEBvRp5j"
      },
      "source": [
        "#passing y_train and y_test to encode_output to be one-hot encoded\n",
        "y_train = encode_output(y_train, eng_vocab_size)\n",
        "y_test = encode_output(y_test, eng_vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmCwWPSjmxgt"
      },
      "source": [
        "#Building the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc-nWGF6QI1T"
      },
      "source": [
        "Here, I have built a simple seq2seq model with LSTMs. The model can be upgraded to a better one that has a better learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFU-1TMmmtEJ"
      },
      "source": [
        "#defining a simple seq2seq model\n",
        "model = Sequential()\n",
        "model.add(Embedding(jp_vocab_size, 256, input_length=jp_len, mask_zero=True))\n",
        "model.add(LSTM(256))\n",
        "model.add(RepeatVector(eng_len))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(eng_vocab_size, activation='softmax')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDWwox-9CmB2"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-QXi6eyCuFz",
        "outputId": "b0cd87a3-dd3f-4917-e6bd-aa4d991e160d"
      },
      "source": [
        "#printing model summary\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 6, 256)            3261184   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 6, 256)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 6, 256)            525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 6, 9621)           2472597   \n",
            "=================================================================\n",
            "Total params: 6,784,405\n",
            "Trainable params: 6,784,405\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbCTgDAzZ5Lh"
      },
      "source": [
        "Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDuPNXzPDt3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f746096-919a-4d3c-eb39-0f9fd0482227"
      },
      "source": [
        "#saving the model\n",
        "model.save(\"model.bin\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model.bin/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model.bin/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnpxRTXtEQ-f"
      },
      "source": [
        "#Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_ai_BVpETZt",
        "outputId": "70dbbeb9-d27c-4350-8aa5-b1dc6cdf4200"
      },
      "source": [
        "#training the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test), verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "255/255 [==============================] - 80s 312ms/step - loss: 4.3459 - accuracy: 0.3574 - val_loss: 5.2643 - val_accuracy: 0.3153\n",
            "Epoch 2/50\n",
            "255/255 [==============================] - 79s 312ms/step - loss: 4.2811 - accuracy: 0.3593 - val_loss: 5.3324 - val_accuracy: 0.3215\n",
            "Epoch 3/50\n",
            "255/255 [==============================] - 79s 311ms/step - loss: 4.2099 - accuracy: 0.3617 - val_loss: 5.3362 - val_accuracy: 0.3221\n",
            "Epoch 4/50\n",
            "255/255 [==============================] - 80s 312ms/step - loss: 4.1394 - accuracy: 0.3630 - val_loss: 5.3487 - val_accuracy: 0.3194\n",
            "Epoch 5/50\n",
            "255/255 [==============================] - 79s 312ms/step - loss: 4.0645 - accuracy: 0.3664 - val_loss: 5.3868 - val_accuracy: 0.3185\n",
            "Epoch 6/50\n",
            "255/255 [==============================] - 80s 313ms/step - loss: 3.9917 - accuracy: 0.3681 - val_loss: 5.4424 - val_accuracy: 0.3186\n",
            "Epoch 7/50\n",
            "255/255 [==============================] - 80s 313ms/step - loss: 3.9199 - accuracy: 0.3710 - val_loss: 5.4873 - val_accuracy: 0.3146\n",
            "Epoch 8/50\n",
            "255/255 [==============================] - 80s 313ms/step - loss: 3.8424 - accuracy: 0.3737 - val_loss: 5.5216 - val_accuracy: 0.3171\n",
            "Epoch 9/50\n",
            "255/255 [==============================] - 80s 312ms/step - loss: 3.7633 - accuracy: 0.3770 - val_loss: 5.5741 - val_accuracy: 0.3152\n",
            "Epoch 10/50\n",
            "255/255 [==============================] - 80s 313ms/step - loss: 3.6806 - accuracy: 0.3813 - val_loss: 5.5649 - val_accuracy: 0.3166\n",
            "Epoch 11/50\n",
            "255/255 [==============================] - 79s 311ms/step - loss: 3.5984 - accuracy: 0.3845 - val_loss: 5.5874 - val_accuracy: 0.3113\n",
            "Epoch 12/50\n",
            "255/255 [==============================] - 80s 312ms/step - loss: 3.5106 - accuracy: 0.3887 - val_loss: 5.6680 - val_accuracy: 0.3048\n",
            "Epoch 13/50\n",
            "255/255 [==============================] - 80s 313ms/step - loss: 3.4270 - accuracy: 0.3942 - val_loss: 5.7026 - val_accuracy: 0.3034\n",
            "Epoch 14/50\n",
            "255/255 [==============================] - 80s 313ms/step - loss: 3.3348 - accuracy: 0.3994 - val_loss: 5.7628 - val_accuracy: 0.3052\n",
            "Epoch 15/50\n",
            "255/255 [==============================] - 80s 313ms/step - loss: 3.2449 - accuracy: 0.4059 - val_loss: 5.7869 - val_accuracy: 0.3055\n",
            "Epoch 16/50\n",
            "255/255 [==============================] - 80s 313ms/step - loss: 3.1536 - accuracy: 0.4138 - val_loss: 5.8320 - val_accuracy: 0.3066\n",
            "Epoch 17/50\n",
            "255/255 [==============================] - 80s 313ms/step - loss: 3.0659 - accuracy: 0.4211 - val_loss: 5.8738 - val_accuracy: 0.3011\n",
            "Epoch 18/50\n",
            "255/255 [==============================] - 80s 312ms/step - loss: 2.9723 - accuracy: 0.4324 - val_loss: 5.8968 - val_accuracy: 0.3083\n",
            "Epoch 19/50\n",
            "255/255 [==============================] - 80s 313ms/step - loss: 2.8891 - accuracy: 0.4434 - val_loss: 5.9378 - val_accuracy: 0.2976\n",
            "Epoch 20/50\n",
            "255/255 [==============================] - 80s 313ms/step - loss: 2.7962 - accuracy: 0.4549 - val_loss: 6.0100 - val_accuracy: 0.2942\n",
            "Epoch 21/50\n",
            "255/255 [==============================] - 80s 313ms/step - loss: 2.7048 - accuracy: 0.4675 - val_loss: 6.0642 - val_accuracy: 0.2933\n",
            "Epoch 22/50\n",
            "255/255 [==============================] - 80s 313ms/step - loss: 2.6211 - accuracy: 0.4811 - val_loss: 6.0962 - val_accuracy: 0.2918\n",
            "Epoch 23/50\n",
            "255/255 [==============================] - 80s 313ms/step - loss: 2.5353 - accuracy: 0.4952 - val_loss: 6.1573 - val_accuracy: 0.2889\n",
            "Epoch 24/50\n",
            "255/255 [==============================] - 80s 313ms/step - loss: 2.4525 - accuracy: 0.5088 - val_loss: 6.1785 - val_accuracy: 0.2905\n",
            "Epoch 25/50\n",
            "255/255 [==============================] - 80s 312ms/step - loss: 2.3701 - accuracy: 0.5212 - val_loss: 6.2330 - val_accuracy: 0.2892\n",
            "Epoch 26/50\n",
            "255/255 [==============================] - 80s 313ms/step - loss: 2.2892 - accuracy: 0.5362 - val_loss: 6.3076 - val_accuracy: 0.2835\n",
            "Epoch 27/50\n",
            "255/255 [==============================] - 80s 314ms/step - loss: 2.2236 - accuracy: 0.5466 - val_loss: 6.3350 - val_accuracy: 0.2886\n",
            "Epoch 28/50\n",
            "255/255 [==============================] - 80s 313ms/step - loss: 2.1480 - accuracy: 0.5600 - val_loss: 6.3690 - val_accuracy: 0.2820\n",
            "Epoch 29/50\n",
            "255/255 [==============================] - 80s 313ms/step - loss: 2.0746 - accuracy: 0.5729 - val_loss: 6.4340 - val_accuracy: 0.2773\n",
            "Epoch 30/50\n",
            "255/255 [==============================] - 80s 312ms/step - loss: 2.0042 - accuracy: 0.5846 - val_loss: 6.4419 - val_accuracy: 0.2812\n",
            "Epoch 31/50\n",
            "255/255 [==============================] - 80s 312ms/step - loss: 1.9400 - accuracy: 0.5958 - val_loss: 6.4941 - val_accuracy: 0.2834\n",
            "Epoch 32/50\n",
            "255/255 [==============================] - 80s 313ms/step - loss: 1.8743 - accuracy: 0.6073 - val_loss: 6.5550 - val_accuracy: 0.2789\n",
            "Epoch 33/50\n",
            "255/255 [==============================] - 79s 311ms/step - loss: 1.8130 - accuracy: 0.6188 - val_loss: 6.5807 - val_accuracy: 0.2833\n",
            "Epoch 34/50\n",
            "255/255 [==============================] - 80s 314ms/step - loss: 1.7572 - accuracy: 0.6284 - val_loss: 6.6655 - val_accuracy: 0.2733\n",
            "Epoch 35/50\n",
            "255/255 [==============================] - 80s 312ms/step - loss: 1.6930 - accuracy: 0.6412 - val_loss: 6.6678 - val_accuracy: 0.2823\n",
            "Epoch 36/50\n",
            "255/255 [==============================] - 79s 311ms/step - loss: 1.6376 - accuracy: 0.6514 - val_loss: 6.7264 - val_accuracy: 0.2714\n",
            "Epoch 37/50\n",
            "255/255 [==============================] - 80s 313ms/step - loss: 1.5776 - accuracy: 0.6633 - val_loss: 6.7583 - val_accuracy: 0.2710\n",
            "Epoch 38/50\n",
            "255/255 [==============================] - 80s 313ms/step - loss: 1.5208 - accuracy: 0.6746 - val_loss: 6.8163 - val_accuracy: 0.2721\n",
            "Epoch 39/50\n",
            "255/255 [==============================] - 78s 308ms/step - loss: 1.4715 - accuracy: 0.6839 - val_loss: 6.8352 - val_accuracy: 0.2743\n",
            "Epoch 40/50\n",
            "255/255 [==============================] - 79s 310ms/step - loss: 1.4151 - accuracy: 0.6963 - val_loss: 6.8878 - val_accuracy: 0.2685\n",
            "Epoch 41/50\n",
            "255/255 [==============================] - 81s 317ms/step - loss: 1.3614 - accuracy: 0.7070 - val_loss: 6.9252 - val_accuracy: 0.2742\n",
            "Epoch 42/50\n",
            "255/255 [==============================] - 81s 317ms/step - loss: 1.3146 - accuracy: 0.7165 - val_loss: 6.9664 - val_accuracy: 0.2725\n",
            "Epoch 43/50\n",
            "255/255 [==============================] - 81s 316ms/step - loss: 1.2748 - accuracy: 0.7239 - val_loss: 7.0075 - val_accuracy: 0.2731\n",
            "Epoch 44/50\n",
            "255/255 [==============================] - 81s 317ms/step - loss: 1.2286 - accuracy: 0.7342 - val_loss: 7.0616 - val_accuracy: 0.2668\n",
            "Epoch 45/50\n",
            "255/255 [==============================] - 81s 316ms/step - loss: 1.1767 - accuracy: 0.7458 - val_loss: 7.1033 - val_accuracy: 0.2673\n",
            "Epoch 46/50\n",
            "255/255 [==============================] - 81s 317ms/step - loss: 1.1335 - accuracy: 0.7560 - val_loss: 7.1372 - val_accuracy: 0.2648\n",
            "Epoch 47/50\n",
            "255/255 [==============================] - 81s 316ms/step - loss: 1.0855 - accuracy: 0.7655 - val_loss: 7.1700 - val_accuracy: 0.2686\n",
            "Epoch 48/50\n",
            "255/255 [==============================] - 81s 316ms/step - loss: 1.0400 - accuracy: 0.7758 - val_loss: 7.1976 - val_accuracy: 0.2690\n",
            "Epoch 49/50\n",
            "255/255 [==============================] - 81s 317ms/step - loss: 0.9951 - accuracy: 0.7852 - val_loss: 7.2787 - val_accuracy: 0.2620\n",
            "Epoch 50/50\n",
            "255/255 [==============================] - 81s 317ms/step - loss: 0.9569 - accuracy: 0.7945 - val_loss: 7.2868 - val_accuracy: 0.2723\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd8b3ed8dd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZP53jJMQfpf"
      },
      "source": [
        "While training each epoch took around 1 minute and 15 seconds, which is why I have kept the epochs to 50. Increasing the number of epochs will increase the accuracy, which is now at 79%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuuAZA7WrtzK"
      },
      "source": [
        "#Model Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwzhqvTgrxIw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "394ed106-7023-4f0b-a556-a8d732646788"
      },
      "source": [
        "#displaying the predictions\n",
        "model.predict(X_test[0].reshape((1, X_train[0].shape[0])))[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.5143189e-07, 1.7986888e-02, 1.3361334e-02, ..., 8.2269719e-11,\n",
              "        3.9959732e-13, 3.3969144e-08],\n",
              "       [8.5694711e-07, 6.8564042e-03, 7.1941578e-04, ..., 1.3771585e-08,\n",
              "        3.6784738e-11, 3.2449901e-08],\n",
              "       [1.0525955e-06, 2.8157169e-03, 1.3400968e-02, ..., 7.6063640e-13,\n",
              "        4.3327755e-10, 2.2884659e-10],\n",
              "       [1.4311839e-06, 1.9597939e-04, 9.7735031e-03, ..., 4.1919295e-14,\n",
              "        2.9205838e-10, 2.1282805e-10],\n",
              "       [9.1502030e-04, 3.6846433e-02, 5.3506184e-05, ..., 2.2830158e-16,\n",
              "        2.4489810e-13, 4.7529145e-09],\n",
              "       [5.9523141e-01, 3.1472158e-02, 4.3592536e-08, ..., 1.2722652e-18,\n",
              "        1.7205449e-17, 2.3052278e-08]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7TPn1m_KSEo"
      },
      "source": [
        "These predictions are tokenized. They need to be mapped to sentences. The following code attempts to achieve that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCIXn4Z0wVcb"
      },
      "source": [
        "#mapping the sequence to sentence\n",
        "def word_to_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIhH3wUq1xMM",
        "outputId": "e2e59baf-38d4-4886-a815-9e4248458eb3"
      },
      "source": [
        "#displaying a source sentece that has been mapped from sequences to sentence\n",
        "sentence = [word_to_id(x, jp_tokenizer) for x in X_train[0]]\n",
        "sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['という', 'こと', 'は', '子供', 'が', 'いる']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5NWHiTC15Rp",
        "outputId": "7c9c56f5-b120-434b-da60-9f88b127890f"
      },
      "source": [
        "#displaying the target sentence\n",
        "tar = [np.argmax(vector) for vector in y_train[0]]\n",
        "tar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[11, 1094, 440, 0, 0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFvnaspf1_oV"
      },
      "source": [
        "#mapping the target language sequence to sentence\n",
        "translation = []\n",
        "for i in tar:\n",
        "    word = word_to_id(i, eng_tokenizer)\n",
        "    if word is None:\n",
        "        break\n",
        "    translation.append(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPPJ6Hk62K3P",
        "outputId": "c3300d88-6d79-4ae8-d291-78a467281df3"
      },
      "source": [
        "#displaying the sentence\n",
        "translation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['and', 'therefore', 'kids']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0i0h2cj2QVx"
      },
      "source": [
        "#predicting the target(english) sequence when fed a source(japanese) sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "    source = source.reshape((1, source.shape[0]))\n",
        "    prediction = model.predict(source, verbose=0)[0]\n",
        "    integers = [np.argmax(vector) for vector in prediction]\n",
        "    target = []\n",
        "    for i in integers:\n",
        "        word = word_to_id(i, tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        target.append(word)\n",
        "    return ' '.join(target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szHRxbn02v4p"
      },
      "source": [
        "#function to map the japanese sequences to sentences\n",
        "def get_japanese(row):\n",
        "    words = [word_to_id(x, jp_tokenizer) for x in row]\n",
        "    words = [word for word in words if word != None]\n",
        "    return ' '.join(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9m_tidi2wqn"
      },
      "source": [
        "#function to map english sequences to sentences\n",
        "def get_english(row):\n",
        "    ints = [np.argmax(vector) for vector in row]\n",
        "    target = []\n",
        "    for i in ints:\n",
        "        word = word_to_id(i, eng_tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        target.append(word)\n",
        "    return ' '.join(target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DRo3QoUN2y5K",
        "outputId": "962e1f13-e433-4752-9481-8c9d8f56a618"
      },
      "source": [
        "#displaying mapped sentence for japanese\n",
        "get_japanese(X_train[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'違う あれ は 。'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IPYc4gyg22CF",
        "outputId": "1f3a27c4-75d1-40ec-caf1-5a42ca9c974a"
      },
      "source": [
        "#displaying the same sentence for english\n",
        "get_english(y_train[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'that is'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HY1YHYjO3BZ4",
        "outputId": "f876b80f-de79-4216-e91d-129b6796d8aa"
      },
      "source": [
        "#displaying the prediction made by the model for the same source sentence\n",
        "predict_sequence(model, eng_tokenizer, X_train[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'that is'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym47WYY2L0PP"
      },
      "source": [
        "From the code above, we can see that the model has made a very accurate prediction. The code below loops through the dataset and gets the model predictions and then outputs the source and target given in the dataset and then what that model predicted for that source sentece."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBX57-PP3F3F",
        "outputId": "f16ac9c7-e109-40ee-baf4-b1af64a26b8b"
      },
      "source": [
        "#looping through the dataset and getting the predictions made by the model\n",
        "for i in range(40):\n",
        "    print(\"The source sentence: \", get_japanese(X_train[i]))\n",
        "    print(\"The translation in target langauge: \", get_english(y_train[i]))\n",
        "    print(\"Prediction made by the model: \", predict_sequence(model, eng_tokenizer, X_train[i]))\n",
        "    print('..........\\nNext Prediction\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The source sentence:  という こと は 子供 が いる\n",
            "The translation in target langauge:  and therefore kids\n",
            "Prediction made by the model:  and therefore kids\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  違う あれ は 。\n",
            "The translation in target langauge:  that is\n",
            "Prediction made by the model:  that is\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  いる ん でしょ\n",
            "The translation in target langauge:  i do\n",
            "Prediction made by the model:  i\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  ありがとう ござい まし た 。\n",
            "The translation in target langauge:  okay thank you\n",
            "Prediction made by the model:  thank you you much\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  よ じゃあ お前 ここ 来い 。\n",
            "The translation in target langauge:  you come here come here\n",
            "Prediction made by the model:  come you come here\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  说起来 你们到 陆地上 感 觉如何\n",
            "The translation in target langauge:  come on\n",
            "Prediction made by the model:  come on\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  いい いい わ よ\n",
            "The translation in target langauge:  thats good thats so good\n",
            "Prediction made by the model:  thats good thats\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  真 の 鉄 島 人 だ\n",
            "The translation in target langauge:  theyre all iron islanders\n",
            "Prediction made by the model:  theyre those iron islanders\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  出 て 行け 出 て 行け\n",
            "The translation in target langauge:  get out\n",
            "Prediction made by the model:  get better\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  助け て 助け て\n",
            "The translation in target langauge:  al you go help may\n",
            "Prediction made by the model:  al you help help\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  いい な 藤崎\n",
            "The translation in target langauge:  good\n",
            "Prediction made by the model:  good\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  あぁ それから サミー・ラニヨン に も\n",
            "The translation in target langauge:  um then theres uh sammy runyon\n",
            "Prediction made by the model:  um then theres uh sammy runyon\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  ローレル から 始めよ う\n",
            "The translation in target langauge:  lets start with laurel\n",
            "Prediction made by the model:  lets start with laurel\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  もう ドラマチック\n",
            "The translation in target langauge:  uhh its just so much drama\n",
            "Prediction made by the model:  uhh its just so much drama\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  つん ぼ か\n",
            "The translation in target langauge:  are you deaf\n",
            "Prediction made by the model:  are you deaf\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  何 か 方法 は 無い か\n",
            "The translation in target langauge:  i need you to do something\n",
            "Prediction made by the model:  you you you do do\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  話 說得有 點小孩 子 氣了\n",
            "The translation in target langauge:  good morning young man\n",
            "Prediction made by the model:  good morning young man\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  私 は 蜜月 と 申し ます\n",
            "The translation in target langauge:  my name is mitsuki\n",
            "Prediction made by the model:  my name is mitsuki\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  あんた と は 違う\n",
            "The translation in target langauge:  different from you\n",
            "Prediction made by the model:  your of you\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  に 変化 し て い ます\n",
            "The translation in target langauge:  and technology is changing very rapidly\n",
            "Prediction made by the model:  and technology this changing changing rapidly\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  の 事 だ ビキニ 環礁 さ\n",
            "The translation in target langauge:  we hear things here\n",
            "Prediction made by the model:  we need you here\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  者 の 一 人 でしょ う\n",
            "The translation in target langauge:  to systematically investigate the human emotions\n",
            "Prediction made by the model:  this systematically investigate the human emotions\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  あなた ホント に 。 あ\n",
            "The translation in target langauge:  you really are\n",
            "Prediction made by the model:  you do do\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  アレン やら ない か\n",
            "The translation in target langauge:  come on allen youre up\n",
            "Prediction made by the model:  come on allen youre up\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  に なっ てる ん です けど\n",
            "The translation in target langauge:  i am hurrying\n",
            "Prediction made by the model:  because heard hurrying\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  クリス 質問 いい でしょ う か\n",
            "The translation in target langauge:  so a question\n",
            "Prediction made by the model:  youre a question\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  ママ って 誰 の こと だ\n",
            "The translation in target langauge:  whos mummy\n",
            "Prediction made by the model:  where mummy\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  飲める じゃ ない か\n",
            "The translation in target langauge:  you really can drink\n",
            "Prediction made by the model:  you really drink drink\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  ほ ー ら 。 ウホホホホホホ 。\n",
            "The translation in target langauge:  i i wonder\n",
            "Prediction made by the model:  i i wonder\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  人 の 血 から 取り入れる\n",
            "The translation in target langauge:  it flows from blood\n",
            "Prediction made by the model:  it flows from blood\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  本当 な の か\n",
            "The translation in target langauge:  this is it\n",
            "Prediction made by the model:  this is it\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  叫ば なかっ た\n",
            "The translation in target langauge:  i didnt scream\n",
            "Prediction made by the model:  i didnt scream\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  が し た 事 は 全て\n",
            "The translation in target langauge:  i was\n",
            "Prediction made by the model:  or was\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  あなた と 同じ だっ た から\n",
            "The translation in target langauge:  but i was like you\n",
            "Prediction made by the model:  the was i judging you\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  一緒 に 来 て\n",
            "The translation in target langauge:  please come with us\n",
            "Prediction made by the model:  let be me us\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  それ は 何\n",
            "The translation in target langauge:  what are all those\n",
            "Prediction made by the model:  what are what doing\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  一体 誰 な ん でしょ う\n",
            "The translation in target langauge:  to ruin these five cermonies\n",
            "Prediction made by the model:  no ruin to to the\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  貿易 に 必要 と さ れる\n",
            "The translation in target langauge:  this vast moneygoround\n",
            "Prediction made by the model:  this vast moneygoround\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  彼 を 殺し た ん だ\n",
            "The translation in target langauge:  it was killing him\n",
            "Prediction made by the model:  i was killing him\n",
            "..........\n",
            "Next Prediction\n",
            "\n",
            "The source sentence:  いいえ 何時も 先 に 帰り ます\n",
            "The translation in target langauge:  did lawson leave with you no\n",
            "Prediction made by the model:  i lawson grow with so no\n",
            "..........\n",
            "Next Prediction\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpoohQXUJmG2"
      },
      "source": [
        "**Conclusion:** From these predictions we can dedue that the model did ok over the test dataset. I seems to give best predictions for smaller sentences as compared to large sentences, where it tends to make mistakes.\n",
        "\n",
        "All in all, it can be said that if the model is trained over a longer period of time, it will produce more accurate predictions"
      ]
    }
  ]
}